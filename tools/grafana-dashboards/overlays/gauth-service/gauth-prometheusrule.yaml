apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    meta.helm.sh/release-name: gauth
    meta.helm.sh/release-namespace: gauth
  name: gauth-prometheus-alerts
  namespace: gauth
spec:
  groups:
  - name: GAUTH
    rules:
    - alert: GAUTH-CPU-Usage
      annotations:
        information: AUTH {{ $labels.pod }} CPU usage is above 300% of CPU Request
          for last 5m at {{ $externalLabels.environment_prometheus }}-{{ $externalLabels.location_prometheus
          }}
        summary: AUTH {{ $labels.pod }} CPU usage alarm at {{ $externalLabels.environment_prometheus
          }}-{{ $externalLabels.location_prometheus }}
      expr: sum(rate(container_cpu_usage_seconds_total{namespace="gauth", pod=~"gauth-auth-.*|gauth-environment-.*|gauth-auth-ui-.*",
        container!="POD"}[1m])) by (pod) > 3
      for: 5m
      labels:
        service: gauth
        severity: HIGH
    - alert: GAUTH-Memory-Usage
      annotations:
        information: AUTH {{ $labels.pod }} Memory usage is above 70% of Memory Limit
          for last 5m at {{ $externalLabels.environment_prometheus }}-{{ $externalLabels.location_prometheus
          }}
        summary: AUTH {{ $labels.pod }} Memory usage alarm at {{ $externalLabels.environment_prometheus
          }}-{{ $externalLabels.location_prometheus }}
      expr: sum(container_memory_usage_bytes{namespace="gauth", pod=~"gauth-auth-.*|gauth-environment-.*|gauth-auth-ui-.*",
        container=""}) by (pod) /sum(container_spec_memory_limit_bytes{namespace="gauth",
        pod=~"gauth-auth-.*|gauth-environment-.*|gauth-auth-ui-.*", container=""})
        by (pod) *100 >70
      for: 5m
      labels:
        service: gauth
        severity: HIGH
    - alert: GAUTH-Pod-NotReady-Count
      annotations:
        information: Deployment {{ $labels.deployment }} has less than or equal 1
          ready pods for a AUTH deployment for last 5m at {{ $externalLabels.environment_prometheus
          }}-{{ $externalLabels.location_prometheus }}
        summary: AUTH  {{ $labels.deployment }} Ready Pods count is 1 for last 5m
          at {{ $externalLabels.environment_prometheus }}-{{ $externalLabels.location_prometheus
          }}
      expr: kube_deployment_status_replicas_available{namespace="gauth", deployment=~"gauth-auth|gauth-environment|gauth-auth-ui"}
        <= 1
      for: 5m
      labels:
        service: gauth
        severity: HIGH
    - alert: GAUTH-Pod-Restarts-Count
      annotations:
        information: AUTH {{ $labels.container }} restart count is above threshold
          for last 5m at {{ $externalLabels.environment_prometheus }}-{{ $externalLabels.location_prometheus
          }}
        summary: AUTH {{ $labels.container }} restart count is above threshold for
          last 5m at {{ $externalLabels.environment_prometheus }}-{{ $externalLabels.location_prometheus
          }}
      expr: sum(increase(kube_pod_container_status_restarts_total{namespace="gauth",
        pod=~"gauth-auth-.*|gauth-environment-.*|gauth-auth-ui-.*"}[5m])) by (container)
        >= 1
      for: 5m
      labels:
        service: gauth
        severity: HIGH
    - alert: GAUTH-Memory-Usage-CRITICAL
      annotations:
        information: AUTH {{ $labels.pod }} Memory usage is above 90% of Memory Limit
          for last 5m at {{ $externalLabels.environment_prometheus }}-{{ $externalLabels.location_prometheus
          }}
        summary: AUTH {{ $labels.pod }} Memory usage alarm at {{ $externalLabels.environment_prometheus
          }}-{{ $externalLabels.location_prometheus }}
      expr: sum(container_memory_usage_bytes{namespace="gauth", pod=~"gauth-auth-.*|gauth-environment-.*|gauth-auth-ui-.*",
        container=""}) by (pod)/sum(container_spec_memory_limit_bytes{namespace="gauth",
        pod=~"gauth-auth-.*|gauth-environment-.*|gauth-auth-ui-.*", container=""})
        by (pod) *100 > 90
      for: 5m
      labels:
        service: gauth
        severity: CRITICAL
    - alert: GAUTH-Pod-Restarts-Count-CRITICAL
      annotations:
        information: AUTH {{ $labels.container }} restart count is above critical
          threshold for last 5m at {{ $externalLabels.environment_prometheus }}-{{
          $externalLabels.location_prometheus }}
        summary: AUTH {{ $labels.container }} restart count is above critical threshold
          for last 5m at {{ $externalLabels.environment_prometheus }}-{{ $externalLabels.location_prometheus
          }}
      expr: sum(increase(kube_pod_container_status_restarts_total{namespace="gauth",
        pod=~"gauth-auth-.*|gauth-environment-.*|gauth-auth-ui-.*"}[5m])) by (container)
        >= 5
      for: 5m
      labels:
        service: gauth
        severity: CRITICAL
    - alert: GAUTH-Pods-NotReady-CRITICAL
      annotations:
        information: Deployment {{ $labels.deployment }}  has 0 ready pods for a AUTH
          deployment for last 1m at {{ $externalLabels.environment_prometheus }}-{{
          $externalLabels.location_prometheus }}
        summary: AUTH {{ $labels.deployment }} Ready Pods count is 0 for last 1m at
          {{ $externalLabels.environment_prometheus }}-{{ $externalLabels.location_prometheus
          }}
      expr: kube_deployment_status_replicas_available{namespace="gauth", deployment=~"gauth-auth|gauth-environment|gauth-auth-ui"}
        == 0
      for: 1m
      labels:
        service: gauth
        severity: CRITICAL
    - alert: auth_jvm_threads_deadlocked
      annotations:
        information: 'AUTH {{ $labels.pod }}. Current value: {{ $value }}'
        summary: Deadlocked jvm threads exist at {{ $labels.pod }}
      expr: jvm_threads_deadlocked{namespace="gauth", job=~"gauth-auth|gauth-environment"}
        > 0
      for: 30s
      labels:
        service: gauth
        severity: CRITICAL
    - alert: auth_high_jvm_gc_pause_seconds_count
      annotations:
        information: 'AUTH {{ $labels.pod }}. Current value: {{ $value }}'
        summary: JVM GC is too often at {{ $labels.pod }}
      expr: delta(jvm_gc_pause_seconds_count{action="end of major GC",namespace="gauth",
        job=~"gauth-auth|gauth-environment"}[1m]) > 10
      for: 30s
      labels:
        service: gauth
        severity: CRITICAL
    - alert: auth_high_5xx_responces_count
      annotations:
        information: 'AUTH {{ $labels.pod }}. Current value: {{ $value }}'
        summary: Too many 5xx responses at {{ $labels.deployment }}
      expr: delta(gws_responses_total{code=~"5..",namespace="gauth", job=~"gauth-auth|gauth-environment"}[1m])
        > 10
      for: 50s
      labels:
        service: gauth
        severity: CRITICAL
    - alert: auth_high_500_responces_count
      annotations:
        information: 'AUTH {{ $labels.pod }}. Current value: {{ $value }}'
        summary: Too many 500 responses at {{ $labels.deployment }}
      expr: delta(gws_responses_total{code=~"500",namespace="gauth", job=~"gauth-auth|gauth-environment"}[1m])
        > 10
      for: 50s
      labels:
        service: gauth
        severity: CRITICAL
    - alert: auth_auth_login_errors
      annotations:
        information: 'Too many login errors for CCID: {{ $labels.contactCenter }}
          at {{ $labels.deployment }}. '
        summary: Too many login errors at {{ $labels.deployment }}
      expr: sum by(contactCenter) (delta(auth_system_login_errors_total{namespace="gauth",
        job=~"gauth-auth"}[1m])) > 20
      for: 60s
      labels:
        service: gauth
        severity: CRITICAL
    - alert: auth_total_count_of_errors_in_PSDK_connections
      annotations:
        information: Total count of errors in PSDK connections is {{ $value }}. Spike
          might indicate a problem with backend or network issue. Shouldn't occur
          too often, so it's one counter for all active connections. Check logs for
          details.
        summary: Total count of errors in PSDK connections at {{ $labels.deployment
          }}
      expr: increase(psdk_conn_error_total{namespace="gauth", job=~"gauth-auth"}[5m])
        > 3
      for: 30s
      labels:
        service: gauth
        severity: HIGH
    - alert: auth_total_count_of_errors_during_context_initialization
      annotations:
        information: Total count of errors during context initialization is {{ $value
          }}. Spike might indicate network or configuration problem. Check logs for
          details.
        summary: Total count of errors during context initialization at {{ $labels.deployment
          }}
      expr: increase(auth_context_error_total{namespace="gauth", job=~"gauth-auth"}[1m])  >
        10
      for: 30s
      labels:
        service: gauth
        severity: HIGH
    - alert: auth_Redis_keys
      annotations:
        information: 'Current number of redis keys is : {{ $value }}'
        summary: High redis keys usage
      expr: azure_RedisCache_totalkeys{instance_name=~".*-gauth-.*"} > 5e+06
      for: 100s
      labels:
        service: gauth
        severity: HIGH
    - alert: auth_Redis_used_memory
      annotations:
        information: Used {{ $value }}% memory at {{ $labels.deployment }}
        summary: High Redis memory usage
      expr: azure_RedisCache_usedmemorypercentage{instance_name=~".*-gauth-.*"} >
        85
      for: 100s
      labels:
        service: gauth
        severity: HIGH
    - alert: auth_saml_response_errors
      annotations:
        information: 'Too many SAML errors for CCID: {{ $labels.contactCenter }} at
          {{ $labels.deployment }}. '
        summary: Too many SAML errors at {{ $labels.deployment }}
      expr: sum by(contactCenter) (delta(auth_saml_response_errors{namespace="gauth",
        job=~"gauth-auth"}[1m])) > 20
      for: 60s
      labels:
        service: gauth
        severity: HIGH
    - alert: auth_saml_timing_errors
      annotations:
        information: 'Too many SAML timing errors for CCID: {{ $labels.contactCenter
          }} at {{ $labels.deployment }}. '
        summary: Too many SAML timming errors at {{ $labels.deployment }}
      expr: sum by(contactCenter) (delta(auth_saml_timing_errors{namespace="gauth",
        job=~"gauth-auth"}[1m])) > 20
      for: 60s
      labels:
        service: gauth
        severity: HIGH
